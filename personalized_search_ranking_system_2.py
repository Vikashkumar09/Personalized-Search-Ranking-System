# -*- coding: utf-8 -*-
"""Personalized search ranking System.2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vb3DfRXvjgJE8IBSyqO7Kb8yWtjAHhZl

Imports + Models
"""

import pandas as pd
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel

device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2").to(device)

"""Embed Functions"""

def embed(texts):
    tokens = tokenizer(
        texts,
        padding=True,
        truncation=True,
        return_tensors="pt"
    ).to(device)

    with torch.no_grad():
        outputs = model(**tokens)

    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()


def embed_in_batches(texts, batch_size=32):
    all_embs = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        all_embs.append(embed(batch))
    return np.vstack(all_embs)

"""Load Guardian Dataset"""

df = pd.read_csv("/content/sample_data/guardian_headlines.csv.zip")

documents = df["Headlines"].dropna().astype(str).tolist()

print("Documents loaded:", len(documents))

"""Limit Size to Avoid RAM Crash"""

documents = documents[:3000]

"""Generate Embeddings"""

doc_embeddings = embed_in_batches(documents, batch_size=16)
print(doc_embeddings.shape)

"""FAISS ANN INDEX (10M+ SCALE READY)"""

pip install faiss-cpu

"""Build the FAISS index (now safe)"""

import faiss

dim = doc_embeddings.shape[1]
index = faiss.IndexHNSWFlat(dim, 32)
index.add(doc_embeddings)

print("FAISS index size:", index.ntotal)

"""Test retrieval"""

def retrieve(query, k=5):
    q_emb = embed([query])
    scores, ids = index.search(q_emb, k)
    return ids[0], scores[0]

ids, scores = retrieve("artificial intelligence")
for i in ids:
    print(documents[i])

"""Cross-Encoder Re-Ranking"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

ce_tokenizer = AutoTokenizer.from_pretrained(
    "cross-encoder/ms-marco-MiniLM-L-6-v2"
)
ce_model = AutoModelForSequenceClassification.from_pretrained(
    "cross-encoder/ms-marco-MiniLM-L-6-v2"
).to(device)

def rerank(query, ids):
    docs = [documents[i] for i in ids]
    pairs = [(query, d) for d in docs]

    tokens = ce_tokenizer(
        pairs, padding=True, truncation=True, return_tensors="pt"
    ).to(device)

    with torch.no_grad():
        scores = ce_model(**tokens).logits.squeeze()

    return scores.cpu().numpy()

ids, _ = retrieve("artificial intelligence", k=50)
ce_scores = rerank("artificial intelligence", ids)

reranked = sorted(
    zip(ids, ce_scores),
    key=lambda x: x[1],
    reverse=True
)

for i, _ in reranked[:5]:
    print(documents[i])

"""Ranking Metrics (NDCG & MRR)"""

from sklearn.metrics import ndcg_score
import numpy as np

y_true = np.random.randint(0, 2, size=len(ce_scores))  # placeholder labels

ndcg = ndcg_score([y_true], [ce_scores])

def mrr(labels):
    for i, l in enumerate(labels):
        if l == 1:
            return 1 / (i + 1)
    return 0

mrr_score = mrr(y_true)

"""XGBoost LambdaRank"""

import xgboost as xgb

X = ce_scores.reshape(-1, 1)
y = y_true
group = [len(X)]

ranker = xgb.XGBRanker(
    objective="rank:ndcg",
    eval_metric="ndcg",
    n_estimators=100
)

ranker.fit(X, y, group=group)

"""GPU Similarity"""

import faiss

if faiss.get_num_gpus() > 0:
    res = faiss.StandardGpuResources()
    index = faiss.index_cpu_to_gpu(res, 0, index)

"""FastAPI (DEPLOYMENT)"""

from fastapi import FastAPI

app = FastAPI()

@app.get("/search")
def search(q: str):
    ids, _ = retrieve(q, k=50)
    scores = rerank(q, ids)
    return [
        {"text": documents[i], "score": float(s)}
        for i, s in zip(ids, scores)
    ]

!uvicorn app:app --reload

"""Create app.py"""

!touch app.py

!ls

!sed -n '1,200p' app.py

from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def root():
    return {"status": "ok"}

!pip install fastapi uvicorn

!python -m uvicorn app:app --reload --port 8000